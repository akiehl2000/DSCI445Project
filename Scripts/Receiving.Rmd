---
title: "Receiving Models"
author: "Adam Kiehl"
date: "11/24/21"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(GGally) # ggpairs() function
library(dlookr) # diagnose() function
library(stats)
library(factoextra) # fivz_eig() function
library(sail)
library(caret) 
library(leaps) # best subset selection
library(glmnet) # LASSO regression
library(tree) # decision tree
library(corrplot) 
library(regclass) #VIF() function
library(gbm) # boosting, bagging, and random forest
library(knitr) # kable() function
library(pryr) # save plots
```

For reproducibility:
```{r}
set.seed(445)
```

# Setup

Read in the data and select relevant fields for training set. 
```{r, message=FALSE}
fileName <- '../Data/masterDF[2018-2020].csv'
rec_data <- read_csv(fileName) %>%
  dplyr::select(-'...1')
rec_data <- rec_data[, c(1:3, 17:23, 51:62)]

train_data <- rec_data[which(rec_data$Rec > 0 & !is.na(rec_data$RecLng)), -c(1:3)]
N <- nrow(train_data)
K <- 10
folds <- sample(1:K, N, replace = TRUE)

for (i in 1:N) {
  if (is.na(train_data$FirstDRec[i])) {
    train_data$FirstDRec[i] <- 0
  }
  if (is.na(train_data$RecPerBr[i])) {
    train_data$RecPerBr[i] <- 0
  }
}
```

Check data frame for integrity. 
```{r}
diagnose(train_data)
```

# Exploratory 

Exploratory correlation matrix and histograms for four response variables of interest. 
```{r}
ggpairs(train_data[,c(2:4, 7)])
```

Correlation matrix and VIF scores used to identify multicollinearity. Evidence of high correlation found between `FirstDRec` and `Tgt` (0.75), `RecYBC` and `FirstDRec` (0.77), `ADOT` and `YBCPerR` (0.86), and `DropPerRec` and `RecDrop` (0.84). 
```{r}
vif <- VIF(lm(Rec ~ . - RecYds - RecTD - FL, train_data)); vif
vif %>%
  kable(format = 'latex') %>%
  saveRDS('../Tables/vifScores.rds')

source("http://www.sthda.com/upload/rquery_cormat.r")
rquery.cormat(train_data[,-c(2:4, 7)])$r
```

Removing `RecLng`, FirstDRec`, `RecYBC`, `YBCPerR`, `DropPerRec`, and `YACPerR` yielded reasonable VIF scores and diangostically semi-sound models, although there is definitely evidence of non-linearity or violated assumptions. 
```{r}
model <- lm(Rec ~ . - RecYds - RecTD - FL - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR - RecLng, train_data)
VIF(model)
summary(model)
plot(model)

model <- lm(RecYds ~ . - Rec - RecTD - FL - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR - RecLng, train_data)
VIF(model)
summary(model)
plot(model)

model <- lm(RecTD ~ . - Rec - RecYds - FL - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR - RecLng, train_data)
VIF(model)
summary(model)
plot(model)
```

# Multiple Linear Regression

Multiple linear regression models with 10-fold cross-validation MSE reported. `RecYAC` removed when predicting `RecYds` since $RecYBC+RecYAC=RecYds$ and `RecYBC` is more predictive than `RecYAC`. 
```{r}
mlr_results <- data.frame(Model = c('Rec', 'RecYds', 'RecTD', 'FL'), MSE = rep(NA, 4))
mlr_mse <- data.frame(k = 1:K, mse1 = rep(NA, K), mse2 = rep(NA, K), mse3 = rep(NA, K), mse4 = rep(NA, K))

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  model <- lm(Rec ~ . - RecYds - RecTD - FL, data = trn)
  model <- lm(Rec ~ . - RecYds - RecTD - FL - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, trn)
  pred <- round(predict(model, vld))
  mlr_mse$mse1[which(mlr_mse$k == k)] <- mean((vld$Rec - pred)^2)
  
  model <- lm(RecYds ~ . - Rec - RecTD - FL - RecYAC, data = trn)
  model <- lm(RecYds ~ . - Rec - RecTD - FL - FirstDRec - RecYAC - YBCPerR - DropPerRec - YACPerR, trn)
  pred <- round(predict(model, vld))
  mlr_mse$mse2[which(mlr_mse$k == k)] <- mean((vld$RecYds - pred)^2)
  
  model <- lm(RecTD ~ . - Rec - RecYds - FL, data = trn)
  model <- lm(RecTD ~ . - Rec - RecYds - FL - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, trn)
  pred <- round(predict(model, vld))
  mlr_mse$mse3[which(mlr_mse$k == k)] <- mean((vld$RecTD - pred)^2)
  
  model <- lm(FL ~ . - Rec - RecYds - RecTD, data = trn)
  model <- lm(FL ~ . - Rec - RecYds - RecTD - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, trn)
  pred <- round(predict(model, vld))
  mlr_mse$mse4[which(mlr_mse$k == k)] <- mean((vld$FL - pred)^2)
}

mlr_results$MSE[which(mlr_results$Model == 'Rec')] <- round(mean(mlr_mse$mse1), 3)
mlr_results$MSE[which(mlr_results$Model == 'RecYds')] <- round(mean(mlr_mse$mse2), 3)
mlr_results$MSE[which(mlr_results$Model == 'RecTD')] <- round(mean(mlr_mse$mse3), 3)
mlr_results$MSE[which(mlr_results$Model == 'FL')] <- round(mean(mlr_mse$mse4), 3)

mlr_results
```

# Best Subset Selection

Best subset selection performed with `Rec` as response. A non-linear relationship with `RecLng` was explored but nothing significant was found. 
```{r}
best_subset <- regsubsets(Rec ~ . - RecYds - RecTD - FL - RecLng - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, train_data, nvmax = 9)
best_subset_sum <- summary(best_subset)

opt <- 4

par(mfrow=c(3,1))
ggplot() + 
  geom_line(aes(1:9, best_subset_sum$cp)) + 
  geom_point(aes(opt, best_subset_sum$cp[opt]), color = 'red') +
  labs(title='Mallows\' Cp',x='Predictors',y='Value')

ggplot() + 
  geom_line(aes(1:9, best_subset_sum$bic)) + 
  geom_point(aes(opt, best_subset_sum$bic[opt]), color = 'red') + 
  labs(title='BIC', x='Predictors', y='Value')

ggplot() + 
  geom_line(aes(1:9, best_subset_sum$adjr2)) + 
  geom_point(aes(opt, best_subset_sum$adjr2[opt]), color = 'red') + 
  labs(title='Adjusted R^2', x='Predictors', y='Value')

coefficients(best_subset, id = as.character(opt))

best_subset_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  best_subset <- lm(Rec ~ Tgt + RecLng + RecYAC + RecDrop + Rat, trn)
  pred <- round(predict(best_subset, vld))
  best_subset_mse[k] <- mean((vld$Rec - pred)^2)
}

recMSE <- round(mean(best_subset_mse), 3)
```

Best subset selection performed with `RecYds` as response.
```{r}
best_subset <- regsubsets(RecYds ~ . - Rec - RecTD - FL - RecLng - FirstDRec - RecYAC - YBCPerR - DropPerRec - YACPerR, train_data, nvmax = 9)
best_subset_sum <- summary(best_subset)

opt <- 3

par(mfrow=c(3,1))
ggplot() + 
  geom_line(aes(1:9, best_subset_sum$cp)) + 
  geom_point(aes(opt, best_subset_sum$cp[opt]), color = 'red') +
  labs(title='Mallows\' Cp',x='Predictors',y='Value')

ggplot() + 
  geom_line(aes(1:9, best_subset_sum$bic)) + 
  geom_point(aes(opt, best_subset_sum$bic[opt]), color = 'red') + 
  labs(title='BIC', x='Predictors', y='Value')

ggplot() + 
  geom_line(aes(1:9, best_subset_sum$adjr2)) + 
  geom_point(aes(opt, best_subset_sum$adjr2[opt]), color = 'red') + 
  labs(title='Adjusted R^2', x='Predictors', y='Value')

coefficients(best_subset, id = as.character(opt))

best_subset_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  best_subset <- lm(RecYds ~ Tgt + RecLng + Rat + I(RecYBC^2), trn)
  pred <- round(predict(best_subset, vld))
  best_subset_mse[k] <- mean((vld$RecYds - pred)^2)
}

recYdsMSE <- round(mean(best_subset_mse), 3)
```

Best subset selection performed with `RecTD` as response. 
```{r}
best_subset <- regsubsets(RecTD ~ . - Rec - RecYds - FL - RecLng - FirstDRec - RecYAC - YBCPerR - DropPerRec - YACPerR, train_data, nvmax = 9)
best_subset_sum <- summary(best_subset)

opt <- 3

par(mfrow=c(3,1))
ggplot() + 
  geom_line(aes(1:9, best_subset_sum$cp)) + 
  geom_point(aes(opt, best_subset_sum$cp[opt]), color = 'red') +
  labs(title='Mallows\' Cp',x='Predictors',y='Value')

ggplot() + 
  geom_line(aes(1:9, best_subset_sum$bic)) + 
  geom_point(aes(opt, best_subset_sum$bic[opt]), color = 'red') + 
  labs(title='BIC', x='Predictors', y='Value')

ggplot() + 
  geom_line(aes(1:9, best_subset_sum$adjr2)) + 
  geom_point(aes(opt, best_subset_sum$adjr2[opt]), color = 'red') + 
  labs(title='Adjusted R^2', x='Predictors', y='Value')

coefficients(best_subset, id = as.character(opt))

best_subset_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  best_subset <- lm(RecTD ~ Tgt + RecInt + Rat, trn)
  pred <- round(predict(best_subset, vld))
  best_subset_mse[k] <- mean((vld$RecTD - pred)^2)
}

recTDMSE <- round(mean(best_subset_mse), 3)
```

Best subset selection performed with `FL` as response. 
```{r}
best_subset <- regsubsets(FL ~ . - Rec - RecYds - RecTD - RecLng  - FirstDRec - RecYAC - YBCPerR - DropPerRec - YACPerR, train_data, nvmax = 9)
best_subset_sum <- summary(best_subset)

opt <- 4

par(mfrow=c(3,1))
ggplot() + 
  geom_line(aes(1:9, best_subset_sum$cp)) + 
  geom_point(aes(opt, best_subset_sum$cp[opt]), color = 'red') +
  labs(title='Mallows\' Cp',x='Predictors',y='Value')

ggplot() + 
  geom_line(aes(1:9, best_subset_sum$bic)) + 
  geom_point(aes(opt, best_subset_sum$bic[opt]), color = 'red') + 
  labs(title='BIC', x='Predictors', y='Value')

ggplot() + 
  geom_line(aes(1:9, best_subset_sum$adjr2)) + 
  geom_point(aes(opt, best_subset_sum$adjr2[opt]), color = 'red') + 
  labs(title='Adjusted R^2', x='Predictors', y='Value')

coefficients(best_subset, id = as.character(opt))

best_subset_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  best_subset <- lm(FL ~ Tgt + Fmb + ADOT + RecDrop, trn)
  pred <- round(predict(best_subset, vld))
  best_subset_mse[k] <- mean((vld$FL - pred)^2)
}

FLMSE <- round(mean(best_subset_mse), 3)
```

10-fold cross validation best subset selection MSEs:
```{r}
best_subset_results <- data.frame(Model = c('Rec', 'RecYds', 'RecTD', 'FL'), MSE = rep(NA, 4))

best_subset_results$MSE[which(best_subset_results$Model == 'Rec')] <- recMSE
best_subset_results$MSE[which(best_subset_results$Model == 'RecYds')] <- recYdsMSE
best_subset_results$MSE[which(best_subset_results$Model == 'RecTD')] <- recTDMSE
best_subset_results$MSE[which(best_subset_results$Model == 'FL')] <- FLMSE

best_subset_results
```


# LASSO Regression

LASSO regression performed with `Rec` as the response. 
```{r}
x <- model.matrix(Rec ~ . - RecYds - RecTD - FL - RecLng - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, train_data)
y <- train_data$Rec

lasso_model <- cv.glmnet(x, y, type.measure = 'mse', nfolds = 10); lasso_model

plot(lasso_model)

lasso_model <- glmnet(x, y, lambda = exp(-2)); lasso_model
coef(lasso_model)

lasso_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  x <- model.matrix(Rec ~ . - RecYds - RecTD - FL - RecLng - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, trn)
  y <- trn$Rec
  
  lasso_model <- glmnet(x, y, lambda = exp(-1))
  
  pred_x <- model.matrix(Rec ~ . - RecYds - RecTD - FL - RecLng - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, vld)
  pred <- round(predict(lasso_model, pred_x))
  
  lasso_mse[k] <- mean((vld$Rec - pred)^2)
}

recMSE <- round(mean(lasso_mse), 3)
```

LASSO regression performed with `RecYds` as the response. 
```{r}
x <- model.matrix(RecYds ~ . - Rec - RecTD - FL - RecLng - FirstDRec - RecYAC - YBCPerR - DropPerRec - YACPerR, train_data)
y <- train_data$RecYds

lasso_model <- cv.glmnet(x, y, type.measure = 'mse', nfolds = 10); lasso_model

plot(lasso_model)

lasso_model <- glmnet(x, y, lambda = exp(1)); lasso_model
coef(lasso_model)

lasso_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  x <- model.matrix(RecYds ~ . - Rec - RecTD - FL - RecLng - FirstDRec - RecYAC - YBCPerR - DropPerRec - YACPerR, trn)
  y <- trn$RecYds
  
  lasso_model <- glmnet(x, y, lambda = exp(1))
  
  pred_x <- model.matrix(RecYds ~ . - Rec - RecTD - FL - RecLng - FirstDRec - RecYAC - YBCPerR - DropPerRec - YACPerR, vld)
  pred <- round(predict(lasso_model, pred_x))
  
  lasso_mse[k] <- mean((vld$RecYds - pred)^2)
}

recYdsMSE <- round(mean(lasso_mse), 3)
```

LASSO regression performed with `RecTD` as the response. 
```{r}
x <- model.matrix(RecTD ~ . - Rec - RecYds - FL - RecLng - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, train_data)
y <- train_data$RecTD

lasso_model <- cv.glmnet(x, y, type.measure = 'mse', nfolds = 10); lasso_model

plot(lasso_model)

lambda <- exp
lasso_model <- glmnet(x, y, lambda = exp(-5)); lasso_model
coef(lasso_model)

lasso_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  x <- model.matrix(RecTD ~ . - Rec - RecYds - FL - RecLng - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, trn)
  y <- trn$RecTD
  
  lasso_model <- glmnet(x, y, lambda = exp(-4))
  
  pred_x <- model.matrix(RecTD ~ . - Rec - RecYds - FL - RecLng - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, vld)
  pred <- round(predict(lasso_model, pred_x))
  
  lasso_mse[k] <- mean((vld$RecTD - pred)^2)
}

recTDMSE <- round(mean(lasso_mse), 3)
```

LASSO regression performed with `FL` as the response. 
```{r}
x <- model.matrix(FL ~ . - Rec - RecYds - RecTD - RecLng - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, train_data)
y <- train_data$FL

lasso_model <- cv.glmnet(x, y, type.measure = 'mse', nfolds = 10); lasso_model

plot(lasso_model)

lasso_model <- glmnet(x, y, lambda = exp(-3)); lasso_model
coef(lasso_model)

lasso_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  x <- model.matrix(FL ~ . - Rec - RecYds - RecTD - RecLng - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, trn)
  y <- trn$FL
  
  lasso_model <- glmnet(x, y, lambda = exp(-4))
  
  pred_x <- model.matrix(FL ~ . - Rec - RecYds - RecTD - RecLng - FirstDRec - RecYBC - YBCPerR - DropPerRec - YACPerR, vld)
  pred <- round(predict(lasso_model, pred_x))
  
  lasso_mse[k] <- mean((vld$FL - pred)^2)
}

FLMSE <- round(mean(lasso_mse), 3)
```

10-fold cross validation LASSO regression MSEs:
```{r}
lasso_results <- data.frame(Model = c('Rec', 'RecYds', 'RecTD', 'FL'), MSE = rep(NA, 4))

lasso_results$MSE[which(lasso_results$Model == 'Rec')] <- recMSE
lasso_results$MSE[which(lasso_results$Model == 'RecYds')] <- recYdsMSE
lasso_results$MSE[which(lasso_results$Model == 'RecTD')] <- recTDMSE
lasso_results$MSE[which(lasso_results$Model == 'FL')] <- FLMSE

lasso_results
```

# Principal Componenets Analysis

Principal component analysis perfromed on training data. 
```{r}
pca_model <- prcomp(train_data[, -c(2:4, 7)], center = TRUE, scale = TRUE)
summary(pca_model)
```

Percentage of total variance explained by each principal component. 
```{r}
fviz_eig(pca_model)
```

Principal component loading matrix. 
```{r}
pca_scores <- data.frame(pca_model$x)
data.frame(pca_model$rotation[, 1:8])
```

# Principal Components Regression

Principal components regression performed on the `Rec` variable. 
```{r}
pca_train <- pca_scores %>%
  mutate(Rec = train_data$Rec)
results <- data.frame(pc = 1:15, mse = rep(NA, 15))

for (pc in results$pc) {
  mse <- rep(NA, K)
  for (k in 1:K) {
    trn <- pca_train[folds != k, c(1:pc, 16)]
    vld <- pca_train[folds == k, c(1:pc, 16)]
    
    pca_model <- lm(Rec ~ ., data = trn)
    pred <- round(predict(pca_model, vld))
    mse[k] <- mean((vld$Rec - pred)^2)
  }
  results$mse[which(results$pc == pc)] <- mean(mse)
}

ggplot(results) +
  geom_line(aes(pc, mse)) +
  geom_point(aes(7, mse[7]), color = 'red') +
  labs(title='10-Fold Cross Validation MSE', x='Principal Components', y='10-Fold MSE')

recMSE <- round(results$mse[which(results$pc == 7)], 3)
```

Principal components regression performed on the `RecYds` variable. 
```{r}
pca_train <- pca_scores %>%
  mutate(RecYds = train_data$RecYds)
results <- data.frame(pc = 1:15, mse = rep(NA, 15))

for (pc in results$pc) {
  mse <- rep(NA, K)
  for (k in 1:K) {
    trn <- pca_train[folds != k, c(1:pc, 16)]
    vld <- pca_train[folds == k, c(1:pc, 16)]
    
    pca_model <- lm(RecYds ~ ., data = trn)
    pred <- round(predict(pca_model, vld))
    mse[k] <- mean((vld$RecYds - pred)^2)
  }
  results$mse[which(results$pc == pc)] <- mean(mse)
}

ggplot(results) +
  geom_line(aes(pc, mse)) +
  geom_point(aes(7, mse[7]), color = 'red') +
  labs(title='10-Fold Cross Validation MSE', x='Principal Components', y='10-Fold MSE')

recYdsMSE <- round(results$mse[which(results$pc == 7)], 3)
```

Principal components regression performed on the `RecTD` variable. 
```{r}
pca_train <- pca_scores %>%
  mutate(RecTD = train_data$RecTD)
results <- data.frame(pc = 1:15, mse = rep(NA, 15))

for (pc in results$pc) {
  mse <- rep(NA, K)
  for (k in 1:K) {
    trn <- pca_train[folds != k, c(1:pc, 16)]
    vld <- pca_train[folds == k, c(1:pc, 16)]
    
    pca_model <- lm(RecTD ~ ., data = trn)
    pred <- round(predict(pca_model, vld))
    mse[k] <- mean((vld$RecTD - pred)^2)
  }
  results$mse[which(results$pc == pc)] <- mean(mse)
}

ggplot(results) +
  geom_line(aes(pc, mse)) +
  geom_point(aes(8, mse[8]), color = 'red') +
  labs(title='10-Fold Cross Validation MSE', x='Principal Components', y='10-Fold MSE')

recTDMSE <- round(results$mse[which(results$pc == 8)], 3)
```

Principal components regression performed on the `FL` variable. 
```{r}
pca_train <- pca_scores %>%
  mutate(FL = train_data$FL)
results <- data.frame(pc = 1:15, mse = rep(NA, 15))

for (pc in results$pc) {
  mse <- rep(NA, K)
  for (k in 1:K) {
    trn <- pca_train[folds != k, c(1:pc, 16)]
    vld <- pca_train[folds == k, c(1:pc, 16)]
    
    pca_model <- lm(FL ~ ., data = trn)
    pred <- round(predict(pca_model, vld))
    mse[k] <- mean((vld$FL - pred)^2)
  }
  results$mse[which(results$pc == pc)] <- mean(mse)
}

ggplot(results) +
  geom_line(aes(pc, mse)) +
  geom_point(aes(7, mse[7]), color = 'red') +
  labs(title='10-Fold Cross Validation MSE', x='Principal Components', y='10-Fold MSE')

FLMSE <- round(results$mse[which(results$pc == 7)], 3)
```

10-fold cross validation PC regression MSEs:
```{r}
pcr_results <- data.frame(Model = c('Rec', 'RecYds', 'RecTD', 'FL'), MSE = rep(NA, 4))

pcr_results$MSE[which(pcr_results$Model == 'Rec')] <- recMSE
pcr_results$MSE[which(pcr_results$Model == 'RecYds')] <- recYdsMSE
pcr_results$MSE[which(pcr_results$Model == 'RecTD')] <- recTDMSE
pcr_results$MSE[which(pcr_results$Model == 'FL')] <- FLMSE

pcr_results
```

# Tree Models

Tree model built to predict `Rec` variable. 
```{r}
tree_model <- tree(Rec ~ . - RecYds - RecTD - FL, train_data)
cv_tree_model <- cv.tree(tree_model)

opt <- 7

ggplot() +
  geom_line(aes(cv_tree_model$size, cv_tree_model$dev)) +
  geom_point(aes(opt, cv_tree_model$dev[which(cv_tree_model$size == opt)]), color = 'red') +
  scale_x_discrete(limits = factor(1:10)) +
  labs(title='CV Decision Tree Deviance', x='Terminal Nodes', y='Error')

tree_model <- prune.tree(tree_model, best = as.character(opt))
summary(tree_model)

plot(tree_model)
text(tree_model)

tree_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  tree_model <- prune.tree(tree(Rec ~ . - RecYds - RecTD - FL, trn), best = as.character(opt))
  
  pred <- round(predict(tree_model, vld))
  
  tree_mse[k] <- mean((vld$Rec - pred)^2)
}

recMSE <- round(mean(tree_mse), 3)
```

Tree model built to predict `RecYds` variable. 
```{r}
tree_model <- tree(RecYds ~ . - Rec - RecTD - FL - RecYAC, train_data)
cv_tree_model <- cv.tree(tree_model)

opt <- 7

ggplot() +
  geom_line(aes(cv_tree_model$size, cv_tree_model$dev)) +
  geom_point(aes(opt, cv_tree_model$dev[which(cv_tree_model$size == opt)]), color = 'red') +
  scale_x_discrete(limits = factor(1:10)) +
  labs(title='CV Decision Tree Deviance', x='Terminal Nodes', y='Error')

tree_model <- prune.tree(tree_model, best = as.character(opt))
summary(tree_model)

plot(tree_model)
text(tree_model)

tree_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  tree_model <- prune.tree(tree(RecYds ~ . - Rec - RecTD - FL - RecYAC, trn), best = as.character(opt))
  
  pred <- round(predict(tree_model, vld))
  
  tree_mse[k] <- mean((vld$RecYds - pred)^2)
}

recYdsMSE <- round(mean(tree_mse), 3)
```

Tree model built to predict `RecTD` variable. 
```{r}
tree_model <- tree(RecTD ~ . - Rec - RecYds - FL, train_data)
cv_tree_model <- cv.tree(tree_model)

opt <- 2

ggplot() +
  geom_line(aes(cv_tree_model$size, cv_tree_model$dev)) +
  geom_point(aes(opt, cv_tree_model$dev[which(cv_tree_model$size == opt)]), color = 'red') +
  scale_x_discrete(limits = factor(1:10)) +
  labs(title='CV Decision Tree Deviance', x='Terminal Nodes', y='Error')

tree_model <- prune.tree(tree_model, best = as.character(opt))
summary(tree_model)

plot(tree_model)
text(tree_model)

tree_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  tree_model <- prune.tree(tree(RecTD ~ . - Rec - RecYds - FL, trn), best = as.character(opt))
  
  pred <- round(predict(tree_model, vld))
  
  tree_mse[k] <- mean((vld$RecTD - pred)^2)
}

recTDMSE <- round(mean(tree_mse), 3)
```

Tree model built to predict `FL` variable. 
```{r}
tree_model <- tree(FL ~ . - Rec - RecYds - RecTD, train_data)
cv_tree_model <- cv.tree(tree_model)

opt <- 2

ggplot() +
  geom_line(aes(cv_tree_model$size, cv_tree_model$dev)) +
  geom_point(aes(opt, cv_tree_model$dev[which(cv_tree_model$size == opt)]), color = 'red') +
  scale_x_discrete(limits = factor(1:10)) +
  labs(title='CV Decision Tree Deviance', x='Terminal Nodes', y='Error')

tree_model <- prune.tree(tree_model, best = as.character(opt))
summary(tree_model)

plot(tree_model)
text(tree_model)

tree_mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]
  
  tree_model <- prune.tree(tree(FL ~ . - Rec - RecYds - RecTD, trn), best = as.character(opt))
  
  pred <- round(predict(tree_model, vld))
  
  tree_mse[k] <- mean((vld$FL - pred)^2)
}

FLMSE <- round(mean(tree_mse), 3)
```

10-fold cross validation pruned tree MSEs:
```{r}
tree_results <- data.frame(Model = c('Rec', 'RecYds', 'RecTD', 'FL'), MSE = rep(NA, 4))

tree_results$MSE[which(tree_results$Model == 'Rec')] <- recMSE
tree_results$MSE[which(tree_results$Model == 'RecYds')] <- recYdsMSE
tree_results$MSE[which(tree_results$Model == 'RecTD')] <- recTDMSE
tree_results$MSE[which(tree_results$Model == 'FL')] <- FLMSE

tree_results
```

# Boosting

Boosted tree model to predict `Rec` variable.
```{r, warning=FALSE, message=FALSE}
results <- data.frame(lambda = seq(0, .5, length.out = 5), MSE = rep(NA, 5))

for (lambda in results$lambda) {
  mse <- rep(NA, K)
  
  for (k in 1:K) {
    trn <- train_data[folds != k,]
    vld <- train_data[folds == k,]
    
    model <- gbm(Rec ~ . - RecYds - RecTD - FL, data = trn, distribution = 'gaussian', n.trees = 100, shrinkage = lambda)
    
    pred <- round(predict(model, vld))
    
    mse[k] <- mean((vld$Rec - pred)^2)
  }
  
  results$MSE[which(results$lambda == lambda)] <- mean(mse)
}

ggplot(results) +
  geom_line(aes(lambda, MSE)) +
  geom_point(aes(.125, results$MSE[which(results$lambda == .125)]), color = 'red') +
  labs(title='Shrinkage Factor Selection', subtitle='10-fold CV MSE')

recMSE <- round(results$MSE[which(results$MSE == min(results$MSE))], 3)
```

Boosted tree model to predict `RecYds` variable.
```{r, warning=FALSE, message=FALSE}
results <- data.frame(lambda = seq(0, .5, length.out = 5), MSE = rep(NA, 5))

for (lambda in results$lambda) {
  mse <- rep(NA, K)
  
  for (k in 1:K) {
    trn <- train_data[folds != k,]
    vld <- train_data[folds == k,]
    
    model <- gbm(RecYds ~ . - Rec - RecTD - FL - RecYAC, data = trn, distribution = 'gaussian', n.trees = 100, shrinkage = lambda)
    
    pred <- round(predict(model, vld))
    
    mse[k] <- mean((vld$RecYds - pred)^2)
  }
  
  results$MSE[which(results$lambda == lambda)] <- mean(mse)
}

ggplot(results) +
  geom_line(aes(lambda, MSE)) +
  geom_point(aes(.125, results$MSE[which(results$lambda == .125)]), color = 'red') +
  labs(title='Shrinkage Factor Selection', subtitle='10-fold CV MSE')

recYdsMSE <- round(results$MSE[which(results$MSE == min(results$MSE))], 3)
```
Boosted tree model to predict `RecTD` variable.
```{r, warning=FALSE, message=FALSE}
results <- data.frame(lambda = seq(0, .5, length.out = 5), MSE = rep(NA, 5))

for (lambda in results$lambda) {
  mse <- rep(NA, K)
  
  for (k in 1:K) {
    trn <- train_data[folds != k,]
    vld <- train_data[folds == k,]
    
    model <- gbm(RecTD ~ . - Rec - RecYds - FL, data = trn, distribution = 'gaussian', n.trees = 100, shrinkage = lambda)
    
    pred <- round(predict(model, vld))
    
    mse[k] <- mean((vld$RecTD - pred)^2)
  }
  
  results$MSE[which(results$lambda == lambda)] <- mean(mse)
}

ggplot(results) +
  geom_line(aes(lambda, MSE)) +
  geom_point(aes(.125, results$MSE[which(results$lambda == .125)]), color = 'red') +
  labs(title='Shrinkage Factor Selection', subtitle='10-fold CV MSE')

recTDMSE <- round(results$MSE[which(results$MSE == min(results$MSE))], 3)
```
Boosted tree model to predict `FL` variable.
```{r, warning=FALSE, message=FALSE}
results <- data.frame(lambda = seq(0, .5, length.out = 5), MSE = rep(NA, 5))

for (lambda in results$lambda) {
  mse <- rep(NA, K)
  
  for (k in 1:K) {
    trn <- train_data[folds != k,]
    vld <- train_data[folds == k,]
    
    model <- gbm(FL ~ . - Rec - RecYds - RecTD, data = trn, distribution = 'gaussian', n.trees = 100, shrinkage = lambda)
    
    pred <- round(predict(model, vld))
    
    mse[k] <- mean((vld$FL - pred)^2)
  }
  
  results$MSE[which(results$lambda == lambda)] <- mean(mse)
}

ggplot(results) +
  geom_line(aes(lambda, MSE)) +
  geom_point(aes(.125, results$MSE[which(results$lambda == .125)]), color = 'red') +
  labs(title='Shrinkage Factor Selection', subtitle='10-fold CV MSE')

FLMSE <- round(results$MSE[which(results$MSE == min(results$MSE))], 3)
```
10-fold cross validation boosted tree MSEs:
```{r}
boost_results <- data.frame(Model = c('Rec', 'RecYds', 'RecTD', 'FL'), MSE = rep(NA, 4))

boost_results$MSE[which(boost_results$Model == 'Rec')] <- recMSE
boost_results$MSE[which(boost_results$Model == 'RecYds')] <- recYdsMSE
boost_results$MSE[which(boost_results$Model == 'RecTD')] <- recTDMSE
boost_results$MSE[which(boost_results$Model == 'FL')] <- FLMSE

boost_results
```

# Bagging

Bagged tree model to predict `Rec`. 
```{r}
model <- randomForest(Rec ~ . - RecYds - RecTD - FL, data = train_data, mtry = 15, ntree = 100)
import <- importance(model)

data.frame(Var = as.vector(labels(import)[[1]]), Purity = as.vector(import)) %>%
  arrange(desc(Purity)) %>%
  mutate(Var = factor(Var, levels = Var)) %>%
  ggplot() +
  geom_col(aes(Var, Purity)) + 
  theme(axis.text.x = element_text(angle = 45)) +
  labs(title = 'Variable Selection with Bagging', y = 'Importance')

mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]

  model <- randomForest(Rec ~ . - RecYds - RecTD - FL, data = trn, mtry = 15, ntree = 100)

  pred <- round(predict(model, vld))

  mse[k] <- mean((vld$Rec - pred)^2)
}

recMSE <- round(mean(mse), 3)
```

Bagged tree model to predict `RecYds`. 
```{r}
model <- randomForest(RecYds ~ . - Rec - RecTD - FL - RecYAC, data = train_data, mtry = 14, ntree = 100)
import <- importance(model)

data.frame(Var = as.vector(labels(import)[[1]]), Purity = as.vector(import)) %>%
  arrange(desc(Purity)) %>%
  mutate(Var = factor(Var, levels = Var)) %>%
  ggplot() +
  geom_col(aes(Var, Purity)) + 
  theme(axis.text.x = element_text(angle = 45)) +
  labs(title = 'Variable Selection with Bagging', y = 'Importance')

mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]

  model <- randomForest(RecYds ~ . - Rec - RecTD - FL - RecYAC, data = trn, mtry = 14, ntree = 100)

  pred <- round(predict(model, vld))

  mse[k] <- mean((vld$RecYds - pred)^2)
}

recYdsMSE <- round(mean(mse), 3)
```

Bagged tree model to predict `RecTD`. 
```{r, warning=FALSE}
model <- randomForest(RecTD ~ . - Rec - RecYds - FL, data = train_data, mtry = 15, ntree = 100)
import <- importance(model)

data.frame(Var = as.vector(labels(import)[[1]]), Purity = as.vector(import)) %>%
  arrange(desc(Purity)) %>%
  mutate(Var = factor(Var, levels = Var)) %>%
  ggplot() +
  geom_col(aes(Var, Purity)) + 
  theme(axis.text.x = element_text(angle = 45)) +
  labs(title = 'Variable Selection with Bagging', y = 'Importance')

mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]

  model <- randomForest(RecTD ~ . - Rec - RecYds - FL, data = trn, mtry = 15, ntree = 100)

  pred <- round(predict(model, vld))

  mse[k] <- mean((vld$RecTD - pred)^2)
}

recTDMSE <- round(mean(mse), 3)
```

Bagged tree model to predict `FL`. 
```{r, warning=FALSE}
model <- randomForest(FL ~ . - Rec - RecYds - RecTD - FL, data = train_data, mtry = 15, ntree = 100)
import <- importance(model)

data.frame(Var = as.vector(labels(import)[[1]]), Purity = as.vector(import)) %>%
  arrange(desc(Purity)) %>%
  mutate(Var = factor(Var, levels = Var)) %>%
  ggplot() +
  geom_col(aes(Var, Purity)) + 
  theme(axis.text.x = element_text(angle = 45)) +
  labs(title = 'Variable Selection with Bagging', y = 'Importance')

mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]

  model <- randomForest(FL ~ . - Rec - RecYds - RecTD - FL, data = trn, mtry = 15, ntree = 100)

  pred <- round(predict(model, vld))

  mse[k] <- mean((vld$FL - pred)^2)
}

FLMSE <- round(mean(mse), 3)
```

10-fold cross validation bagged tree MSEs:
```{r}
bag_results <- data.frame(Model = c('Rec', 'RecYds', 'RecTD', 'FL'), MSE = rep(NA, 4))

bag_results$MSE[which(bag_results$Model == 'Rec')] <- recMSE
bag_results$MSE[which(bag_results$Model == 'RecYds')] <- recYdsMSE
bag_results$MSE[which(bag_results$Model == 'RecTD')] <- recTDMSE
bag_results$MSE[which(bag_results$Model == 'FL')] <- FLMSE

bag_results
```

# Random Forest

Random forest model to predict `Rec`. 
```{r}
model <- randomForest(Rec ~ . - RecYds - RecTD - FL, data = train_data, mtry = 4, ntree = 100)
import <- importance(model)

data.frame(Var = as.vector(labels(import)[[1]]), Purity = as.vector(import)) %>%
  arrange(desc(Purity)) %>%
  mutate(Var = factor(Var, levels = Var)) %>%
  ggplot() +
  geom_col(aes(Var, Purity)) + 
  theme(axis.text.x = element_text(angle = 45)) +
  labs(title = 'Variable Selection with Random Forest', y = 'Importance')

mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]

  model <- randomForest(Rec ~ . - RecYds - RecTD - FL, data = trn, mtry = 4, ntree = 100)

  pred <- round(predict(model, vld))

  mse[k] <- mean((vld$Rec - pred)^2)
}

recMSE <- round(mean(mse), 3)
```

Random forest model to predict `RecYds`. 
```{r}
model <- randomForest(RecYds ~ . - Rec - RecTD - FL - RecYAC, data = train_data, mtry = 4, ntree = 100)
import <- importance(model)

data.frame(Var = as.vector(labels(import)[[1]]), Purity = as.vector(import)) %>%
  arrange(desc(Purity)) %>%
  mutate(Var = factor(Var, levels = Var)) %>%
  ggplot() +
  geom_col(aes(Var, Purity)) + 
  theme(axis.text.x = element_text(angle = 45)) +
  labs(title = 'Variable Selection with Random Forest', y = 'Importance')

mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]

  model <- randomForest(RecYds ~ . - Rec - RecTD - FL - RecYAC, data = trn, mtry = 4, ntree = 100)

  pred <- round(predict(model, vld))

  mse[k] <- mean((vld$RecYds - pred)^2)
}

recYdsMSE <- round(mean(mse), 3)
```

Random forest model to predict `RecTD`. 
```{r, warning=FALSE}
model <- randomForest(RecTD ~ . - Rec - RecYds - FL, data = train_data, mtry = 4, ntree = 100)
import <- importance(model)

data.frame(Var = as.vector(labels(import)[[1]]), Purity = as.vector(import)) %>%
  arrange(desc(Purity)) %>%
  mutate(Var = factor(Var, levels = Var)) %>%
  ggplot() +
  geom_col(aes(Var, Purity)) + 
  theme(axis.text.x = element_text(angle = 45)) +
  labs(title = 'Variable Selection with Random Forest', y = 'Importance')

mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]

  model <- randomForest(RecTD ~ . - Rec - RecYds - FL, data = trn, mtry = 4, ntree = 100)

  pred <- round(predict(model, vld))

  mse[k] <- mean((vld$RecTD - pred)^2)
}

recTDMSE <- round(mean(mse), 3)
```

Random forest model to predict `FL`. 
```{r, warning=FALSE}
model <- randomForest(FL ~ . - Rec - RecYds - RecTD - FL, data = train_data, mtry = 4, ntree = 100)
import <- importance(model)

data.frame(Var = as.vector(labels(import)[[1]]), Purity = as.vector(import)) %>%
  arrange(desc(Purity)) %>%
  mutate(Var = factor(Var, levels = Var)) %>%
  ggplot() +
  geom_col(aes(Var, Purity)) + 
  theme(axis.text.x = element_text(angle = 45)) +
  labs(title = 'Variable Selection with Random Forest', y = 'Importance')

mse <- rep(NA, K)

for (k in 1:K) {
  trn <- train_data[folds != k,]
  vld <- train_data[folds == k,]

  model <- randomForest(FL ~ . - Rec - RecYds - RecTD - FL, data = trn, mtry = 4, ntree = 100)

  pred <- round(predict(model, vld))

  mse[k] <- mean((vld$FL - pred)^2)
}

FLMSE <- round(mean(mse), 3)
```

10-fold cross validation bagged tree MSEs:
```{r}
rf_results <- data.frame(Model = c('Rec', 'RecYds', 'RecTD', 'FL'), MSE = rep(NA, 4))

rf_results$MSE[which(rf_results$Model == 'Rec')] <- recMSE
rf_results$MSE[which(rf_results$Model == 'RecYds')] <- recYdsMSE
rf_results$MSE[which(rf_results$Model == 'RecTD')] <- recTDMSE
rf_results$MSE[which(rf_results$Model == 'FL')] <- FLMSE

rf_results
```

# Results

MSE results using standardized seed-determined 10-fold training cross validation procedure. 
```{r}
results <- data.frame(Model = c('Rec', 'RecYds', 'RecTD', 'FL'),
                      MLR = mlr_results$MSE,
                      Subset = best_subset_results$MSE,
                      LASSO = lasso_results$MSE,
                      PCR = pcr_results$MSE,
                      Tree = tree_results$MSE,
                      Boost = boost_results$MSE,
                      Bag = bag_results$MSE,
                      RF = rf_results$MSE); results

results[1:3, ] %>%
  kable(format = 'latex') %>%
  saveRDS('../Tables/recResults.rds')

results[1:3, 1:6] %>%
  kable(format = 'latex') %>%
  saveRDS('../Tables/recResultsStart.rds')

results[1:3, c(1, 7:9)] %>%
  kable(format = 'latex') %>%
  saveRDS('../Tables/recResultsEnd.rds')
```

Save trained best models for testing. 
```{r, warning=FALSE}
set.seed(445)

# Random forest model chosen for Rec
randomForest(Rec ~ Tgt + FirstDRec + RecYAC + RecYBC, data = train_data, mtry = 2, ntree = 200) %>%
  saveRDS('../Models/recModel.rds')

# Bagged tree model chosen for RecYds
randomForest(RecYds ~ RecYBC + FirstDRec + RecLng + YACPerR, data = train_data, mtry = 4, ntree = 200) %>%
  saveRDS('../Models/recYdsModel.rds')

# Bagged tree model chosen for RecTD
randomForest(RecTD ~ Rat, data = train_data, mtry = 1, ntree = 200) %>%
  saveRDS('../Models/recTDModel.rds')

# Decision tree model chosen for FL
prune.tree(tree(FL ~ Fmb, train_data), best = '2') %>%
  saveRDS('../Models/recFLModel.rds')
```
